{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30840349",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526f64e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeiras linhas do dataset:\n",
      "           Time           Source     Destination Protocol  Length  \\\n",
      "0   7675.538342   35.157.240.102   192.168.1.158  TLSv1.2     875   \n",
      "1  20371.259680  192.168.100.111   147.32.57.107      TCP      60   \n",
      "2  26145.265144  192.168.100.111   112.28.138.95      TCP      60   \n",
      "3  54364.382321    192.168.1.158  35.157.240.102      TCP      60   \n",
      "4  56559.046524    192.168.1.158  35.157.240.102      TCP      60   \n",
      "\n",
      "                                                Info  Label  \n",
      "0                                   Application Data      0  \n",
      "1              24  >  23 [SYN] Seq=0 Win=14600 Len=0      1  \n",
      "2           27824  >  81 [SYN] Seq=0 Win=14600 Len=0      1  \n",
      "3  [TCP Dup ACK 76575#1] 28291  >  443 [ACK] Seq=...      0  \n",
      "4  30367  >  443 [ACK] Seq=2189969 Ack=5662275 Wi...      0  \n",
      "\n",
      "Colunas disponíveis: ['Time', 'Source', 'Destination', 'Protocol', 'Length', 'Info', 'Label']\n",
      "\n",
      "Distribuição da coluna 'Label':\n",
      "Label\n",
      "0    303605\n",
      "1    303605\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Valores em falta por coluna:\n",
      "Time           0\n",
      "Source         0\n",
      "Destination    0\n",
      "Protocol       0\n",
      "Length         0\n",
      "Info           0\n",
      "Label          0\n",
      "dtype: int64\n",
      "\n",
      "Treino 1 de 3\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Guilherme\\Desktop\\Redes de dados\\Projeto\\venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m30361/30361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 885us/step - accuracy: 0.6614 - loss: 0.5999 - val_accuracy: 0.6677 - val_loss: 0.5886\n",
      "Epoch 2/15\n",
      "\u001b[1m30084/30361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 716us/step - accuracy: 0.6696 - loss: 0.5871"
     ]
    }
   ],
   "source": [
    "# 1. Carregar o Dataset e Exploração Inicial\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "# Caminho para o ficheiro (ajusta conforme o teu caso)\n",
    "ficheiro = r\"C:\\Users\\Guilherme\\Desktop\\Redes de dados\\Projeto\\Dataset\\Dataset2\\dataset2_ircBot_hajime.xlsx\"\n",
    "\n",
    "# Carregar o datasetS\n",
    "df = pd.read_excel(ficheiro)\n",
    "\n",
    "# Mostra as primeiras \n",
    "print(\"Primeiras linhas do dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# Verificar as colunas disponíveis\n",
    "print(\"\\nColunas disponíveis:\", list(df.columns))\n",
    "\n",
    "# Análise de classes (para confirmar se está equilibrado)\n",
    "print(\"\\nDistribuição da coluna 'Label':\")\n",
    "print(df['Label'].value_counts())\n",
    "\n",
    "# Verificar tipos de dados e valores nulos\n",
    "print(\"\\nValores em falta por coluna:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# 2. Pré-processamento dos Dados\n",
    "\n",
    "# (Opcional) Codificar 'Protocol' se existir\n",
    "if 'Protocol' in df.columns:\n",
    "    df['Protocol_num'] = LabelEncoder().fit_transform(df['Protocol'])\n",
    "    features = ['Length', 'Protocol_num']\n",
    "else:\n",
    "    features = ['Length']\n",
    "\n",
    "# Definir X e y\n",
    "X = df[features].values\n",
    "y = df['Label'].values\n",
    "\n",
    "# Normalizar as features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Guardar scaler\n",
    "joblib.dump(scaler, \"scaler_ann_iot23.gz\")\n",
    "\n",
    "# Dividir treino/validação (80%/20%)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "# 3. Criar e Treinar a ANN\n",
    "\n",
    "def criar_modelo(input_dim, n_neuronios=16):\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Dense(n_neuronios, activation='relu', input_shape=(input_dim,)),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Parâmetros do modelo\n",
    "NUM_TREINOS = 3\n",
    "NUM_EPOCAS = 15\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# Guardar históricos e métricas\n",
    "historicos = []\n",
    "scores = []\n",
    "for i in range(NUM_TREINOS):\n",
    "    print(f\"\\nTreino {i+1} de {NUM_TREINOS}\")\n",
    "    modelo = criar_modelo(X_train.shape[1])\n",
    "    historico = modelo.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=NUM_EPOCAS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        verbose=1\n",
    "    )\n",
    "    historicos.append(historico)\n",
    "    # Previsão em validação\n",
    "    y_pred = (modelo.predict(X_val) > 0.5).astype(int).flatten()\n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    scores.append(acc)\n",
    "    print(f\"Accuracy do treino {i+1}: {acc:.4f}\")\n",
    "    print(\"Classification Report:\\n\", classification_report(y_val, y_pred))\n",
    "\n",
    "# 4. Visualização dos Resultados\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i, h in enumerate(historicos):\n",
    "    plt.plot(h.history['val_accuracy'], label=f'Treino {i+1}', alpha=0.6)\n",
    "plt.title('Accuracy de Validação por Época')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Accuracy média e desvio padrão\n",
    "print(\"\\nAccuracies em cada treino:\", scores)\n",
    "print(\"Accuracy média:\", np.mean(scores))\n",
    "print(\"Accuracy desvio padrão:\", np.std(scores))\n",
    "\n",
    "# Avaliação detalhada do último treino\n",
    "y_pred_final = (modelo.predict(X_val) > 0.5).astype(int).flatten()\n",
    "print(\"\\nMatriz de Confusão (último treino):\\n\", confusion_matrix(y_val, y_pred_final))\n",
    "\n",
    "# 5. Guardar Modelo Treinado\n",
    "\n",
    "modelo.save(\"modelo_ann_iot23.h5\")\n",
    "print(\"\\nModelo guardado em 'modelo_ann_iot23.h5'\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4beb7a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Códigos dos protocolos: {'ARP': np.int64(0), 'BT-DHT': np.int64(1), 'CDP': np.int64(2), 'DHCP': np.int64(3), 'DNS': np.int64(4), 'HTTP': np.int64(5), 'ICMP': np.int64(6), 'ICMPv6': np.int64(7), 'IGMPv2': np.int64(8), 'LOOP': np.int64(9), 'NTP': np.int64(10), 'SSDP': np.int64(11), 'SSH': np.int64(12), 'SSHv2': np.int64(13), 'TCP': np.int64(14), 'TELNET': np.int64(15), 'TLSv1.2': np.int64(16), 'UAUDP': np.int64(17), 'UDP': np.int64(18)}\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
      "\n",
      "Exemplo 1: Length=60, Protocol=UDP (18) => Predição: [0]\n",
      "  → O modelo detetou: **BENIGNO** (0)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\n",
      "Exemplo 2: Length=1200, Protocol=TCP (14) => Predição: [1]\n",
      "  → O modelo detetou: **ATAQUE** (1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\n",
      "Exemplo 3: Length=250, Protocol=HTTP (5) => Predição: [1]\n",
      "  → O modelo detetou: **ATAQUE** (1)\n"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# 7. Simulação IDS — Teste com nomes reais de protocolo\n",
    "# =============================================\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Carregar o modelo e scaler treinados\n",
    "modelo = load_model(\"modelo_ann_iot23.h5\")\n",
    "scaler = joblib.load(\"scaler_ann_iot23.gz\")\n",
    "\n",
    "ficheiro = r\"C:\\Users\\Guilherme\\Desktop\\Redes de dados\\Projeto\\Dataset\\Dataset2\\dataset2_ircBot_hajime.xlsx\"\n",
    "df = pd.read_excel(ficheiro)\n",
    "\n",
    "# Criar o LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(df['Protocol'])  \n",
    "\n",
    "# Mostrar os códigos dos protocolos\n",
    "protocol_map = dict(zip(encoder.classes_, encoder.transform(encoder.classes_)))\n",
    "print(\"Códigos dos protocolos:\", protocol_map)\n",
    "\n",
    "# Exemplos reais de previsão\n",
    "exemplos = [\n",
    "    {'length': 60, 'protocol': 'UDP'},\n",
    "    {'length': 1200, 'protocol': 'TCP'}, \n",
    "    {'length': 250, 'protocol': 'HTTP'}\n",
    "]\n",
    "\n",
    "# Prever vários exemplos\n",
    "for i, ex in enumerate(exemplos):\n",
    "    length_exemplo = ex['length']\n",
    "    protocolo_exemplo = ex['protocol']\n",
    "    if protocolo_exemplo in protocol_map:\n",
    "        protocol_num = protocol_map[protocolo_exemplo]\n",
    "        novos_dados = np.array([[length_exemplo, protocol_num]])\n",
    "        novos_dados_norm = scaler.transform(novos_dados)\n",
    "        y_pred = (modelo.predict(novos_dados_norm) > 0.5).astype(int)\n",
    "        print(f\"\\nExemplo {i+1}: Length={length_exemplo}, Protocol={protocolo_exemplo} ({protocol_num}) => Predição:\", y_pred.flatten())\n",
    "        if y_pred[0][0] == 1:\n",
    "            print(\"  → O modelo detetou: **ATAQUE** (1)\")\n",
    "        else:\n",
    "            print(\"  → O modelo detetou: **BENIGNO** (0)\")\n",
    "    else:\n",
    "        print(f\"\\nExemplo {i+1}: Protocol '{protocolo_exemplo}' não encontrado no encoding! Tenta outro protocolo.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
